{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Denoising Variational Lossy Autoencoder<br>\n",
    "We will create a ladder variational autoencoder or lvae (<code>lvae</code>), an autoregressive decoder (<code>noise_model</code>) and a deterministic decoder (<code>s_decoder</code>). Our goal is to remove the noise from our data, $\\mathbf{x}$, revealing an estimate of the underlying clean signal, $\\mathbf{s}$. In this example, the data is corrupted by noise that is correlated along rows.\n",
    "\n",
    "\n",
    "The lvae</code> and autoregressive decoder will work together to train a latent variable model of the distribution over the noisy data, with latent variables produced by the lvae and then decoded into a model of the data distribution by the autoregressive decoder. The autoregressive decoder will have a 1-dimensional receptive field, oriented horizontally, allowing it accurately model the noise component of the data distribution, hence the name <code>noise_model</code>, but not the signal component. The lvae will therefore produce latent variables containing only signal content.\n",
    "\n",
    "The deterministic decoder will learn to take these latent variables and map them back into image space, hence the name <code>s_decoder</code>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import tifffile\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from lvae.models.lvae import LadderVAE\n",
    "from noise_model.pixelcnn import PixelCNN\n",
    "from s_decoder import SDecoder\n",
    "from dvlae import DVLAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We will be using the C. Majalis dataset, first published in: <br>\n",
    "Broaddus, C., Krull, A., Weigert, M., Schmidt, U. and Myers, G., 2020, April. Removing structured noise with self-supervised blind-spot networks. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI) (pp. 159-163). IEEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for our data.\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "# check if data has been downloaded already\n",
    "data_path = \"data/flower.tif\"\n",
    "if not os.path.exists(data_path):\n",
    "    urllib.request.urlretrieve(\"https://download.fht.org/jug/n2v/flower.tif\", data_path)\n",
    "\n",
    "# load the data\n",
    "low_snr = tifffile.imread(data_path).astype(float)\n",
    "low_snr = torch.from_numpy(low_snr).to(torch.float32)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and validation dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDatasetUnsupervised(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, n_iters=1, transform=None):\n",
    "        self.images = images\n",
    "        self.n_images = len(images)\n",
    "        self.n_iters = n_iters\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_images * self.n_iters\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % self.n_images\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>batch_size</code> Number of images in a training batch <br>\n",
    "<code>crop_size</code> The data will be randomly cropped during training. This specifies the size of that crop. Reduce if images are smaller than 256x256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "crop_size = 256\n",
    "n_iters = (low_snr[0].shape[-1] * low_snr[0].shape[-2]) // crop_size**2\n",
    "transform = transforms.RandomCrop(crop_size)\n",
    "\n",
    "low_snr = low_snr[torch.randperm(len(low_snr))]\n",
    "train_set = low_snr[: int(len(low_snr) * 0.9)]\n",
    "val_set = low_snr[int(len(low_snr) * 0.9) :]\n",
    "\n",
    "train_set = TrainDatasetUnsupervised(train_set, n_iters=n_iters, transform=transform)\n",
    "val_set = TrainDatasetUnsupervised(val_set, n_iters=n_iters, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=False, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the models\n",
    "<code>lvae</code> The ladder variational autoencoder that will output latent variables.<br>\n",
    "* <code>s_code_channels</code> Number of channels in outputted latent variable. Set to 64 for reduced memory consumption.\n",
    "* <code>n_layers</code> Number of levels in the hierarchical vae. Set to 6 for reduced memory consumption.\n",
    "* <code>z_dims</code> the numer of latent space dimensionas at each level of the hierarchy.\n",
    "\n",
    "<code>noise_model</code> The autoregressive decoder that will decoder the latent variables into a distribution over the input.<br>\n",
    "* <code>kernel_size</code> Length of 1D convolutional kernels.\n",
    "* <code>RF_shape</code> Whether the receptive field should be oriented \"horizontal\" or \"vertical\", to match the orientation of the noise.\n",
    "* <code>n_filters</code> Number of feature channels.\n",
    "* <code>n_out_layers</code> Number of final 1x1 convolutions.\n",
    "* <code>n_gaussians</code> Number of components in Gaussian mixture used to model data.\n",
    "\n",
    "<code>s_decoder</code> A decoder that will map the latnet variables into image space. <br>\n",
    "<code>dvlae</code> The backbone that will unify and train the above three models.\n",
    "* <code>n_grad_batches</code> Number of batches to accumulate gradients for before updating weights of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_code_channels = 128\n",
    "\n",
    "n_layers = 14\n",
    "z_dims = [s_code_channels // 2] * n_layers\n",
    "if n_layers == 14:\n",
    "    downsampling = [0, 1] * (n_layers // 2)\n",
    "elif n_layers <= 7:\n",
    "    downsampling = [1] * n_layers\n",
    "\n",
    "lvae = LadderVAE(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    img_shape=(crop_size, crop_size),\n",
    "    s_code_channels=s_code_channels,\n",
    "    n_filters=s_code_channels,\n",
    "    z_dims=z_dims,\n",
    "    downsampling=downsampling,\n",
    ")\n",
    "\n",
    "noise_model = PixelCNN(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    s_code_channels=s_code_channels,\n",
    "    kernel_size=5,\n",
    "    RF_shape=\"horizontal\",\n",
    "    n_filters=64,\n",
    "    n_layers=4,\n",
    "    n_out_layers=1,\n",
    "    n_gaussians=3,\n",
    ")\n",
    "\n",
    "s_decoder = SDecoder(\n",
    "    colour_channels=low_snr.shape[1], s_code_channels=s_code_channels, n_filters=s_code_channels\n",
    ")\n",
    "\n",
    "dvlae = DVLAE(\n",
    "    vae=lvae,\n",
    "    noise_model=noise_model,\n",
    "    s_decoder=s_decoder,\n",
    "    data_mean=low_snr.mean(),\n",
    "    data_std=low_snr.std(),\n",
    "    n_grad_batches=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"convallaria\"\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=checkpoint_path,\n",
    "    accelerator=\"gpu\" if use_cuda else \"cpu\",\n",
    "    devices=1,\n",
    "    max_epochs=1000,\n",
    "    log_every_n_steps=len(train_set) // batch_size,\n",
    "    callbacks=[EarlyStopping(patience=100, monitor=\"val/sd_loss\")],\n",
    ")\n",
    "\n",
    "trainer.fit(dvlae, train_loader, val_loader)\n",
    "trainer.save_checkpoint(os.path.join(checkpoint_path, \"final_model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
