{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference with COSDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load a trained model and use it to denoise the low signal-to-noise data. We'll then use reference high signal-to-noise data to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import tifffile\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as PSNR\n",
    "\n",
    "import utils\n",
    "from dvlae import DVLAE\n",
    "\n",
    "logger = logging.getLogger('pytorch_lightning')\n",
    "logger.setLevel(logging.WARNING)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Load test data\n",
    "The images that we want to denoise are loaded here. Since high signal-to-noise ratio reference images are available for this dataset, we'll load those too.<br>\n",
    "For the Actin-Confocal dataset, we follow original authors and use the last 8 images as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_snr_path = \"/group/dl4miacourse/image_regression/mito-confocal/mito-confocal-lowsnr.tif\"\n",
    "high_snr_path = \"/group/dl4miacourse/image_regression/mito-confocal/mito-confocal-highsnr.tif\"\n",
    "\n",
    "low_snr = tifffile.imread(low_snr_path)[-8:, None]\n",
    "low_snr = torch.from_numpy(low_snr.astype(float)).to(torch.float)\n",
    "high_snr = tifffile.imread(high_snr_path)[-8:, None]\n",
    "print(low_snr.shape)\n",
    "\n",
    "# The high snr reference images for this dataset are on a different scale to the low snr images.\n",
    "# We will scale and shift the reference images to match the noisy images.\n",
    "high_snr = utils.minimise_mse(high_snr, low_snr.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with training, data should be a `torch.Tensor` with dimensions: [Number of images, Channels, Height, Width] with data type float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Create prediction dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_batch_size` Number of denoised images to produce at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch_size = 1\n",
    "\n",
    "predict_set = utils.PredictDataset(low_snr)\n",
    "predict_loader = torch.utils.data.DataLoader(\n",
    "    predict_set,\n",
    "    batch_size=predict_batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Task 1\n",
    "\n",
    "We need to know what name the model was given to load it. Look at Part 5 of the training.ipynb notebook and find the value we need to give for `model_name` to load the model trained there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ...   ### Insert a string here\n",
    "# model_name = \"mito-confocal\"\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "\n",
    "dvlae = DVLAE.load_from_checkpoint(os.path.join(checkpoint_path, \"final_model.ckpt\"))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if use_cuda else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Denoise\n",
    "In this section, we will look at how COSDD does inference. <br>\n",
    "\n",
    "The model denoises images randomly, giving us a different output each time. First, we will compare two randomly sampled denoised images for the same noisy image. Then, we will produce a single consensus estimate by averaging 100 randomly sampled denoised images. Finally, if the direct denoiser was trained in the previous step, we will see how it can be used to estimate this average in a single pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Random sampling \n",
    "First, we will denoise each image six times and look at the difference between each estimate. The output of the model is stored in the `samples` variable. This has dimensions [Number of images, Sample index, Channels, Height, Width] where different denoised samples for the same image are stored along sample index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = 6\n",
    "\n",
    "dvlae.direct_pred = use_direct_denoiser\n",
    "\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = trainer.predict(dvlae, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Here, we look at the original noisy image, the six denoised estimates and the reference high snr image. Change the value for `img_idx` to look at different images and change values for `top`, `bottom`, `left` and `right` to adjust the crop.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = np.percentile(low_snr.numpy(), 1)\n",
    "vmax = np.percentile(low_snr.numpy(), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
    "ax[0, 0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0, 0].set_title(\"Input\")\n",
    "for i in range(n_samples):\n",
    "    ax[(i + 1) // 4, (i + 1) % 4].imshow(\n",
    "        samples[img_idx][i][crop], vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    ax[(i + 1) // 4, (i + 1) % 4].set_title(f\"Sample {i+1}\")\n",
    "ax[1, 3].imshow(high_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[1, 3].set_title(\"Reference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The six sampled denoised images have subtle differences that express the uncertainty involved in this denoising problem. We can use the reference high snr data to compare their Peak Signal-to-Noise Ration (PSNR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_samples):\n",
    "    psnrs = []\n",
    "    for j in range(len(low_snr)):\n",
    "        gt = high_snr[j].squeeze()\n",
    "        test = samples[j, i].numpy().squeeze()\n",
    "\n",
    "        data_range = np.max(gt) - np.min(gt)\n",
    "\n",
    "        psnrs.append(PSNR(gt, test, data_range=data_range.item()))\n",
    "\n",
    "    print(f\"PSNR sample {i}: {np.mean(psnrs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MMSE estimate\n",
    "\n",
    "In the next cell, we sample many denoised images and average them for the minimum mean square estimate (MMSE). The averaged images will be stored in the `MMSEs` variable, which has the same dimensions as `low_snr`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3\n",
    "Set `n_samples` to 100 to average 100 images, or a different value to average a different number. Then visually inspeect the results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = ...   ### Insert an integer here\n",
    "# n_samples = 100\n",
    "\n",
    "dvlae.direct_pred = use_direct_denoiser\n",
    "\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = trainer.predict(dvlae, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1)\n",
    "MMSEs = torch.mean(samples, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "ax[0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].imshow(samples[img_idx][0][crop], vmin=vmin, vmax=vmax)\n",
    "ax[1].set_title(\"Sample\")\n",
    "ax[2].imshow(MMSEs[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[2].set_title(\"MMSE\")\n",
    "ax[3].imshow(high_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[3].set_title(\"Reference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MMSE will usuallty be closer to the reference than an individual sample and would score a higher PSNR, although it will also be blurrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnrs = []\n",
    "for j in range(len(low_snr)):\n",
    "    gt = high_snr[j].squeeze()\n",
    "    test = MMSEs[j].numpy().squeeze()\n",
    "\n",
    "    data_range = np.max(gt) - np.min(gt)\n",
    "\n",
    "    psnrs.append(PSNR(gt, test, data_range=data_range.item()))\n",
    "\n",
    "print(f\"PSNR MMSE: {np.mean(psnrs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Direct denoising\n",
    "Sampling 100 images and averaging them is a very time consuming. If the direct denoiser was trained in a previous step, it can be used to directly output what the average denoised image would be for a given noisy image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Set `use_direct_denoiser` to `True` to use the Direct Denoiser for inference instead of taking random samples, then visually inspect the results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = ...   ### Insert a boolean here\n",
    "# use_direct_denoiser = True\n",
    "dvlae.direct_pred = use_direct_denoiser\n",
    "\n",
    "direct = trainer.predict(dvlae, predict_loader)\n",
    "direct = torch.cat(direct, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "ax[0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].imshow(direct[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[1].set_title(\"Direct\")\n",
    "ax[2].imshow(MMSEs[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[2].set_title(\"MMSE\")\n",
    "ax[3].imshow(high_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[3].set_title(\"Reference\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PSNR of the direct estimate is often higher than the PSNR of the average of 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnrs = []\n",
    "for j in range(len(low_snr)):\n",
    "    gt = high_snr[j].squeeze()\n",
    "    test = direct[j].numpy().squeeze()\n",
    "\n",
    "    data_range = np.max(gt) - np.min(gt)\n",
    "\n",
    "    psnrs.append(PSNR(gt, test, data_range=data_range.item()))\n",
    "\n",
    "print(f\"PSNR direct: {np.mean(psnrs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "### Checkpoint 2\n",
    "Continue to the next notebook, generation.ipynb\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autonoise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
