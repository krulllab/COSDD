{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference with COSDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load a trained model and use it to denoise the low signal-to-noise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from models.lvae import LadderVAE\n",
    "from models.pixelcnn import PixelCNN\n",
    "from models.s_decoder import SDecoder\n",
    "from models.unet import UNet\n",
    "from models.hub import Hub\n",
    "\n",
    "logger = logging.getLogger('pytorch_lightning')\n",
    "logger.setLevel(logging.WARNING)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load test data\n",
    "The images that we want to denoise are loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "low_snr = utils.load_data(paths=\"./data\",\n",
    "                          patterns=\"actin-confocal-lowsnr.tif\",\n",
    "                          axes=\"SYX\",\n",
    "                          n_dimensions=2)\n",
    "print(f\"Noisy data size: {low_snr.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Create prediction dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_batch_size` (int): Number of denoised images to produce at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch_size = 1\n",
    "\n",
    "predict_set = utils.PredictDataset(low_snr)\n",
    "predict_loader = torch.utils.data.DataLoader(\n",
    "    predict_set,\n",
    "    batch_size=predict_batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we initialise all the model components again. It is important to use the same hyper-parameters that were used in training.ipynb.  Then, the parameters of the model trained in the previous notebook are loaded by setting `model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = (256, 256)\n",
    "\n",
    "dimensions = 2\n",
    "s_code_channels = 64\n",
    "\n",
    "n_layers = 8\n",
    "z_dims = [s_code_channels // 2] * n_layers\n",
    "max_size = max(crop_size)\n",
    "num_halves = math.floor(math.log2(max_size)) - 1\n",
    "downsampling = [1] * n_layers\n",
    "difference = max(n_layers - num_halves, 0)\n",
    "i = 0\n",
    "while difference > 0:\n",
    "    for j in range(n_layers // 2):\n",
    "        downsampling[i + j * 2] = 0\n",
    "        difference -= 1\n",
    "        if difference == 0:\n",
    "            break\n",
    "    i += 1\n",
    "downsampling = [0, 1] * (n_layers // 2)\n",
    "lvae = LadderVAE(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    img_size=crop_size,\n",
    "    s_code_channels=s_code_channels,\n",
    "    n_filters=s_code_channels,\n",
    "    z_dims=z_dims,\n",
    "    downsampling=downsampling,\n",
    "    dimensions=dimensions,\n",
    ")\n",
    "\n",
    "ar_decoder = PixelCNN(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    s_code_channels=s_code_channels,\n",
    "    kernel_size=5,\n",
    "    noise_direction=\"x\", \n",
    "    n_filters=64,\n",
    "    n_layers=4,\n",
    "    n_gaussians=5,\n",
    "    dimensions=dimensions,\n",
    ")\n",
    "\n",
    "s_decoder = SDecoder(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    s_code_channels=s_code_channels,\n",
    "    n_filters=s_code_channels,\n",
    "    dimensions=dimensions,\n",
    ")\n",
    "\n",
    "use_direct_denoiser = True\n",
    "if use_direct_denoiser:\n",
    "    direct_denoiser = UNet(\n",
    "        colour_channels=low_snr.shape[1],\n",
    "        n_filters=s_code_channels,\n",
    "        n_layers=n_layers,\n",
    "        downsampling=downsampling,\n",
    "        loss_fn=\"MSE\",\n",
    "        dimensions=dimensions,\n",
    "    )\n",
    "else:\n",
    "    direct_denoiser = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"actin-confocal\"\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "hub = Hub.load_from_checkpoint(os.path.join(checkpoint_path, \"final_model.ckpt\"),\n",
    "                               vae=lvae, \n",
    "                               s_decoder=s_decoder,\n",
    "                               ar_decoder=ar_decoder,\n",
    "                               direct_denoiser=direct_denoiser)\n",
    "\n",
    "predictor = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    "    enable_checkpointing=False, \n",
    "    logger=False,\n",
    "    precision=\"bf16-mixed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Denoise\n",
    "In this section, we will look at how COSDD does inference. <br>\n",
    "\n",
    "The model denoises images randomly, giving us a different output each time. First, we will compare seven randomly sampled denoised images for the same noisy image. Then, we will produce a single consensus estimate by averaging 100 randomly sampled denoised images. Finally, if the direct denoiser was trained in the previous step, we will see how it can be used to estimate this average in a single pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Random sampling \n",
    "First, we will denoise each image seven times and look at the difference between each estimate. The output of the model is stored in the `samples` variable. This has dimensions [Number of images, Sample index, Channels, Z | Y | X] where different denoised samples for the same image are stored along sample index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = 7\n",
    "\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = predictor.predict(hub, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll look at the original noisy image and the seven denoised estimates. Change the value for `img_idx` to look at different images and change values for `top`, `bottom`, `left` and `right` to adjust the crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = np.percentile(low_snr.numpy(), 1)\n",
    "vmax = np.percentile(low_snr.numpy(), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
    "ax[0, 0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0, 0].set_title(\"Input\")\n",
    "for i in range(n_samples):\n",
    "    ax[(i + 1) // 4, (i + 1) % 4].imshow(\n",
    "        samples[img_idx][i][crop], vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    ax[(i + 1) // 4, (i + 1) % 4].set_title(f\"Sample {i+1}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The six sampled denoised images have subtle differences that express the uncertainty involved in this denoising problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 MMSE estimate\n",
    "\n",
    "In the next cell, we sample many denoised images and average them for the minimum mean square estimate (MMSE). The averaged images will be stored in the `MMSEs` variable, which has the same dimensions as `low_snr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = 100  \n",
    "\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = predictor.predict(hub, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1).half()\n",
    "MMSEs = torch.mean(samples, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].imshow(samples[img_idx][0][crop], vmin=vmin, vmax=vmax)\n",
    "ax[1].set_title(\"Sample\")\n",
    "ax[2].imshow(MMSEs[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[2].set_title(\"MMSE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MMSE will usually be closer to the reference than an individual sample and would score a higher PSNR, although it will also be blurrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Direct denoising\n",
    "Sampling 100 images and averaging them is a very time consuming. If the direct denoiser was trained in a previous step, it can be used to directly output what the average denoised image would be for a given noisy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = True\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "direct = predictor.predict(hub, predict_loader)\n",
    "direct = torch.cat(direct, dim=0).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].imshow(direct[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[1].set_title(\"Direct\")\n",
    "ax[2].imshow(MMSEs[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[2].set_title(\"MMSE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autonoise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
