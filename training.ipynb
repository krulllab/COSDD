{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training COSDD<br>\n",
    "In this notebook, we will train a model to remove row correlated and signal-dependent imaging noise. \n",
    "You will load noisy data and examine the noise for spatial correlation, then initialise a model and monitor its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import tifffile\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from lvae.models.lvae import LadderVAE\n",
    "from ar_decoder.pixelcnn import PixelCNN\n",
    "from s_decoder import SDecoder\n",
    "from direct_denoiser.models.unet import UNet\n",
    "from dvlae import DVLAE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Download and load data\n",
    "\n",
    "In this example we will be using the Mito-Confocal dataset, provided by: <br>\n",
    "Hagen, G.M., Bendesky, J., Machado, R., Nguyen, T.A., Kumar, T. and Ventura, J., 2021. Fluorescence microscopy datasets for training deep neural networks. GigaScience, 10(5), p.giab032."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1\n",
    "\n",
    "The following cell downloads the low signal-to-noise ratio data and corresponding high signal-to-noise ratio data and saves them to `lowsnr_path` and `highsnr_path` as tif files. Use the function `tifffile.imread` to load the data as a numpy array, then add a channel dimension. Finally, convert it into a pytorch tensor with data type float32.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for our data.\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "\n",
    "# check if data has been downloaded already\n",
    "lowsnr_path = \"data/mito-confocal-lowsnr.tif\"\n",
    "if not os.path.exists(lowsnr_path):\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://ftp.cngb.org/pub/gigadb/pub/10.5524/100001_101000/100888/03-mito-confocal/mito-confocal-lowsnr.tif\",\n",
    "        lowsnr_path,\n",
    "    )\n",
    "highsnr_path = \"data/mito-confocal-highsnr.tif\"\n",
    "if not os.path.exists(highsnr_path):\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://ftp.cngb.org/pub/gigadb/pub/10.5524/100001_101000/100888/03-mito-confocal/mito-confocal-highsnr.tif\",\n",
    "        highsnr_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data here\n",
    "# low_snr = ...\n",
    "\n",
    "low_snr = tifffile.imread(lowsnr_path).astype(float)\n",
    "low_snr = low_snr[:, np.newaxis]\n",
    "low_snr = torch.from_numpy(low_snr).float()\n",
    "\n",
    "print(f\"Noisy data dimensions: {low_snr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should have dimensions: [Number of images, Channels, Height, Width]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Examine spatial correlation of the noise\n",
    "\n",
    "This denoiser can remove noise that is correlated along rows or columns of pixels (or not spatially correlated at all). Row- (or column-)correlated noise means that knowing the noise value in one pixel can help us guess the noise value in other pixels in the same row (or column), preventing us from using Noise2Void. Noise correlated along rows of pixels is common in scanning imaging techniques such as scanning confocal microscopy and scanning electron microscopy.\n",
    "\n",
    "To remove this type of noise, we need to tell the denoiser whether the noise is correlated along rows or along columns. We can establish this by estimating the autocorrelation of the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = np.percentile(low_snr, 0.1)\n",
    "vmax = np.percentile(low_snr, 99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(low_snr[0, 0], vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2\n",
    "\n",
    "To estimate the spatial correlation of the noise, we need some samples of pure noise. These can be patches of `low_snr` with no signal. Use `plt.imshow` to plot different slices of `low_snr` and identify some suitable dark patches, then append them to the `dark_patches` list. The more dark patches we have, the more accurate our estimate of the spatial autocorrelation will be. Two example dark patches have already been given.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explore the data here\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(low_snr[1, 0, 200:400, :300], vmin=vmin, vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_patches = [\n",
    "    low_snr[1, 0, 200:400, :300],\n",
    "    low_snr[2, 0, :390, 650:],\n",
    "    ...,  ### add more patches here\n",
    "    # low_snr[5, 0, :200, :400],\n",
    "    # low_snr[6, 0, 300:400, :450]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll pass the `dark_patches` to the `utils.autocorrelation` function and look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_ac = utils.autocorrelation(dark_patches, max_lag=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(noise_ac, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Autocorrelation of the noise\")\n",
    "plt.xlabel(\"Vertical lag\")\n",
    "plt.ylabel(\"Horizontal lag\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot should reveal that each noise pixel is positively correlated with the pixel horizontally adjacent to it. To remove this type of noise, the autoregressive decoder of our VAE must have a receptive field spanning pixels in the same row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Create training and validation dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be fed to the model by two dataloaders, `train_loader` and `val_loader`, for the training and validation set respectively. <br>\n",
    "In this example, 90% of images will be used for training and the remaining 10% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the choice of data augmentation. Operations such as rotating and mirroring will change the orientation of noise structures. Only use operations that won't affect the autocorrelation plotted above. In this example, we just use `transforms.RandomCrop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_size` Number of images in a training batch. <br>\n",
    "`crop_size` The size of randomly cropped patches. Should be less than the dimensions of your images.<br>\n",
    "`train_split` Fraction of images to be used in the training set, with the remainder used for the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "crop_size = 256\n",
    "train_split = 0.9\n",
    "\n",
    "n_iters = (low_snr[0].shape[-1] * low_snr[0].shape[-2]) // crop_size**2\n",
    "transform = transforms.RandomCrop(crop_size)\n",
    "\n",
    "low_snr = low_snr[torch.randperm(len(low_snr))]\n",
    "train_set = low_snr[: int(len(low_snr) * train_split)]\n",
    "val_set = low_snr[int(len(low_snr) * train_split) :]\n",
    "\n",
    "train_set = utils.TrainDataset(train_set, n_iters=n_iters, transform=transform)\n",
    "val_set = utils.TrainDataset(val_set, n_iters=n_iters, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=False, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Create the model\n",
    "\n",
    "The model we will train to denoise consists of four modules, with forth being the optional Direct Denoiser which we can train if we want to speed up inference. Each module is listed below with an explanation of their hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lvae` The ladder variational autoencoder that will output latent variables.<br>\n",
    "* `s_code_channels` (int) Number of channels in outputted latent variable.\n",
    "* `n_layers` (int) Number of levels in the hierarchical vae.\n",
    "* `z_dims` (list(int)) List with the numer of latent space dimensions at each level of the hierarchy. List starts from the input/output level and works down.\n",
    "* `downsampling` (list(int)) Binary list of whether to downsample at each level of the hierarchy. 1 for do and 0 for don't.\n",
    "\n",
    "`ar_decoder` The autoregressive decoder that will decode latent variables into a distribution over the input.<br>\n",
    "* `kernel_size` (int) Length of 1D convolutional kernels.\n",
    "* `RF_shape` (str) Whether the receptive field should be oriented `\"horizontal\"` or `\"vertical\"`. This needs to match the orientation of the noise structures we revealed in the autocorrelation plot in Part 2.\n",
    "* `n_filters` (int) Number of feature channels.\n",
    "* `n_gaussians` (int) Number of components in Gaussian mixture used to model data.\n",
    "\n",
    "`s_decoder` A decoder that will map the latnet variables into image space. <br>\n",
    "* `n_filters` (int) The number of feature channels.<br>\n",
    "\n",
    "`direct_denoiser` The U-Net that can optionally be trained to predict the MMSE or MMAE of the denoised images. This will slow training slightly but massively speed up inference and is worthwile if you have an inference dataset in the gigabytes. See [this paper](https://arxiv.org/abs/2310.18116). Enable or disable the direct denoiser by setting `use_direct_denoiser` to `True` or `False`.\n",
    "* `start_filters` (int) Feature channels at first level of UNet. Doubles at subsequent layers.\n",
    "* `kernel_size` (int) Height and width of kernels.\n",
    "* `depth` (int) Depth of the U-Net.\n",
    "* `loss_fn` (str) Whether to use `\"L1\"` or `\"L2\"` loss function to predict either the mean or pixel-wise median of denoised images respectively.\n",
    "\n",
    "`dvlae` The container that will unify and train the above models.\n",
    "* `n_grad_batches` (int) Number of batches to accumulate gradients for before updating weights of all models. If the batch or random crop size has been reduced to lower memory consumption, increase this value for the effective batch size to stay the same.\n",
    "\n",
    "Note that some hyperparameters have been set to low values for reduced memory consumption. To use the full size model, use the value suggested in the inline comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3\n",
    "\n",
    "Most hyperparameters have been set to recommended values for the small sized model (see comments for values for full sized model). The two that have been left blank are `RF_shape` under the `ar_decoder` and `use_direct_denoiser`. Use the above description of what each hyperparameter means to determine the best value for each of these.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_code_channels = 32  # Set this to 128 to use the full size model\n",
    "\n",
    "n_layers = 6  # Set this to 14 to use the full size model\n",
    "z_dims = [s_code_channels // 2] * n_layers\n",
    "downsampling = [1] * n_layers  # Set this to [0, 1] * (n_layers // 2) when using the full size model\n",
    "lvae = LadderVAE(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    img_shape=(crop_size, crop_size),\n",
    "    s_code_channels=s_code_channels,\n",
    "    n_filters=s_code_channels,\n",
    "    z_dims=z_dims,\n",
    "    downsampling=downsampling,\n",
    ")\n",
    "\n",
    "ar_decoder = PixelCNN(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    s_code_channels=s_code_channels,\n",
    "    kernel_size=5,\n",
    "    RF_shape=...,   ### Insert a value here\n",
    "    n_filters=64,\n",
    "    n_layers=4,\n",
    "    n_gaussians=5,\n",
    ")\n",
    "\n",
    "s_decoder = SDecoder(\n",
    "    colour_channels=low_snr.shape[1],\n",
    "    s_code_channels=s_code_channels,\n",
    "    n_filters=s_code_channels,\n",
    ")\n",
    "\n",
    "use_direct_denoiser = ...  ### Insert a value here\n",
    "if use_direct_denoiser:\n",
    "    direct_denoiser = UNet(\n",
    "        colour_channels=low_snr.shape[1],\n",
    "        n_filters=s_code_channels,\n",
    "        n_layers=n_layers,\n",
    "        downsampling=downsampling,\n",
    "        loss_fn=\"L2\",\n",
    "    )\n",
    "else:\n",
    "    direct_denoiser = None\n",
    "\n",
    "dvlae = DVLAE(\n",
    "    vae=lvae,\n",
    "    ar_decoder=ar_decoder,\n",
    "    s_decoder=s_decoder,\n",
    "    direct_denoiser=direct_denoiser,\n",
    "    data_mean=low_snr.mean(),\n",
    "    data_std=low_snr.std(),\n",
    "    n_grad_batches=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Train the model\n",
    "\n",
    "`model_name` should be set to something appropriate so that the trained parameters can be used later for inference.<br>\n",
    "`max_epochs` The number of training epochs.<br>\n",
    "`patience` If the validation loss has plateaued for this many epochs, training will stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logs can be monitored on Tensorboard. Run the three cells below to activate it in the notebook. Alternatively, open a terminal, activate an environment with Tensorboard installed and enter `tensorboard --logdir path/to/COSDD/checkpoints` then open a browser and enter localhost:6006.\n",
    "\n",
    "In the SCALARS tab, there will be 4 metrics to track (5 if direct denoiser is enabled). These are:<br>\n",
    "1. `kl_loss` The Kullback-Leibler divergence between the VAE's approximate posterior and its prior. This can be thought of as a measure of how much information about the input image is going into the VAE's latent variables. We want information about the input's underlying clean signal to go into the latent variables, so this metric shouldn't go all the way to zero. Instead, it can typically go either up or down during training before plateauing.<br>\n",
    "2. `reconstruction_loss` The negative log-likelihood of the AR decoder's predicted distribution given the input data. This is how accurately the AR decoder is able to predict the input. This value can go below zero and should decrease throughout training before plateauing.<br>\n",
    "3. `elbo` The Evidence Lower Bound, which is the total loss of the main VAE. This is the sum of the kl reconstruction loss and should decrease throughout training before plateauing.<br>\n",
    "4. `sd_loss` The mean squared error between the noisy image and the image predicted by the signal decoder. This metric should steadily decrease towards zero without ever reaching it. Sometimes the loss will not go down for the first few epochs because its input (produced by the VAE) is rapidly changing. This is ok and the loss should start to decrease when the VAE stabilises. <br>\n",
    "5. `dd_loss` The mean squared error between the output of the direct denoiser and the clean images predicted by the signal decoder. This will only be present if `use_direct_denoiser` is set to `True`. The metric should steadily decrease towards zero without ever reaching it, but may be unstable at the start of training as its targets (produced by the signal decoder) are rapidly changing.\n",
    "\n",
    "There will also be an IMAGES tab. This shows noisy input images from the validation set and some outputs. These will be two randomly sampled denoised images (sample 1 and sample 2), the average of ten denoised images (mmse) and if the direct denoiser is enabled, its output (direct estimate).\n",
    "\n",
    "If noise has not been fully removed from the output images, try increasing `n_gaussians` argument of the AR decoder. This will give it more flexibility to model complex noise characteristics. However, setting the value too high can lead to unstable training. Typically, values from 3 to 5 work best.\n",
    "\n",
    "Note that the trainer is set to train for only 20 minutes in this example. Remove the line with `max_time` to train fully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Now the model is ready to start training. Run the following cells and open Tensorboard to monitor logs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mito-confocal\"\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "logger = TensorBoardLogger(checkpoint_path)\n",
    "\n",
    "max_epochs = 1000\n",
    "patience = 100\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    accelerator=\"gpu\" if use_cuda else \"cpu\",\n",
    "    devices=1,\n",
    "    max_epochs=max_epochs,\n",
    "    max_time=\"00:00:20:00\",  # Remove this time limit to train the model fully\n",
    "    log_every_n_steps=len(train_set) // batch_size,\n",
    "    callbacks=[EarlyStopping(patience=patience, monitor=\"val/elbo\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(dvlae, train_loader, val_loader)\n",
    "trainer.save_checkpoint(os.path.join(checkpoint_path, \"final_model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "### Checkpoint 1\n",
    "Continue to the next notebook, prediction.ipynb\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autonoise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
